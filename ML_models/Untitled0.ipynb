{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP96lz9FulPFxD5fA2cGRqL"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"obxVFk561B-e","executionInfo":{"status":"ok","timestamp":1624730799971,"user_tz":-60,"elapsed":4880,"user":{"displayName":"naim mhedhbi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFfR6yw4SzjRo72NjYLc0wwqC8NZ5-SEeMpcMU1A=s64","userId":"06089257256759509045"}},"outputId":"5846dea9-98a8-4ffa-9930-3c1f24702560"},"source":["import warnings\n","warnings.filterwarnings('ignore')\n","import nltk\n","nltk.download('stopwords')\n","# Modules for data manipulation\n","import numpy as np\n","import pandas as pd\n","import re\n","\n","# Modules for visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sb\n","\n","# Tools for preprocessing input data\n","from bs4 import BeautifulSoup\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","# Tools for creating ngrams and vectorizing input data\n","from gensim.models import Word2Vec, Phrases\n","\n","# Tools for building a model\n","from sklearn.model_selection import train_test_split\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM, Dropout, Bidirectional\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing.sequence import pad_sequences\n","\n","# Tools for assessing the quality of model prediction\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","\n","import os\n","from sklearn.model_selection import KFold, cross_val_score, train_test_split\n","import pandas as pd\n","import os\n","import io\n","import numpy as np\n","import zipfile\n","import sys\n","import datetime\n","from sklearn.model_selection import train_test_split\n","import requests\n","import shutil\n","import keras\n","from keras import optimizers\n","from keras import backend as K\n","from keras import regularizers\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout, Flatten\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n","\n","from keras.preprocessing import sequence\n","from keras.preprocessing.text import Tokenizer\n","from keras.callbacks import EarlyStopping\n","\n","from tqdm import tqdm\n","from nltk.corpus import stopwords\n","from nltk.tokenize import RegexpTokenizer \n","import os, re, csv, math, codecs\n","\n","\n","\n","MAX_NB_WORDS = 100000\n","tokenizer = RegexpTokenizer(r'\\w+')\n","stop_words = set(stopwords.words('english'))\n","stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n","\n","from subprocess import check_output\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rLc86jKO2_hA"},"source":["def load_dataset(url: str, sep: str = ',', decode: str = 'utf-8', keep_default_na: bool = True, error_bad_lines: bool = True):\n","    def requests_csv(url: str, decode: str):\n","        return (requests.get(url).content).decode(decode)\n","    \n","    return pd.read_csv(io.StringIO(requests_csv(url, decode)),\n","                       sep=sep,\n","                       keep_default_na=keep_default_na,\n","                       error_bad_lines=error_bad_lines\n","                       )\n","url_dataset = \"https://media.githubusercontent.com/media/Y4rd13/datasets/main/upwork_dataset.csv\" \n","\n","from sklearn.model_selection import train_test_split\n","\n","df =  load_dataset(url_dataset, decode=\"cp1252\", keep_default_na=True, error_bad_lines=True)\n","#train_df = train_df.sample(2000)\n","df['jobText'] = df['jobText'].astype(str) \n","df['approved'] = df['approved'].astype(int)\n","train_df, test_df = train_test_split(df, test_size = 0.2, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"htwU2678CD8e"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kw2bC3xX6NId","colab":{"base_uri":"https://localhost:8080/","height":367},"executionInfo":{"status":"error","timestamp":1624730801623,"user_tz":-60,"elapsed":708,"user":{"displayName":"naim mhedhbi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFfR6yw4SzjRo72NjYLc0wwqC8NZ5-SEeMpcMU1A=s64","userId":"06089257256759509045"}},"outputId":"f6ece7a4-b370-4726-f132-14682a57867b"},"source":["\n","#load embeddings\n","print('loading word embeddings...')\n","embeddings_index = {}\n","f = codecs.open('/content/drive/MyDrive/Andres Giovanna FRL BERT /wiki.simple.vec', encoding='utf-8')\n","for line in tqdm(f):\n","    values = line.rstrip().rsplit(' ')\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","print('found %s word vectors' % len(embeddings_index))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["loading word embeddings...\n"],"name":"stdout"},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-5ebb93436049>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading word embeddings...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Andres Giovanna FRL BERT /wiki.simple.vec'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, encoding, errors, buffering)\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;31m# Force opening of the file in binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Andres Giovanna FRL BERT /wiki.simple.vec'"]}]},{"cell_type":"code","metadata":{"id":"H_Sn9Jn4Clqt"},"source":["print(\"num train: \", train_df.shape[0])\n","print(\"num test: \", test_df.shape[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_jWgfhsdGGn6"},"source":["train_labels = train_df['approved'].values \n","test_labels = test_df['approved'].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3O6XO_l9CvXn"},"source":["import seaborn as sns\n","y_train = np.asarray(train_labels).astype('float32').reshape((-1,1))\n","y_test = np.asarray(test_labels).astype('float32').reshape((-1,1))\n","\n","\n","#visualize word distribution\n","train_df['doc_len'] = train_df['jobText'].apply(lambda words: len(words.split(\" \")))\n","max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\n","sns.distplot(train_df['doc_len'], hist=True, kde=True, color='b', label='doc len')\n","plt.axvline(x=max_seq_len, color='k', linestyle='--', label='max len')\n","plt.title('comment length'); plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"StSCg0wqFEp_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0K9Ke_-9DJLW"},"source":["raw_docs_train = train_df['jobText'].tolist()\n","raw_docs_test = test_df['jobText'].tolist() \n","num_classes = 2\n","\n","print(\"pre-processing train data...\")\n","processed_docs_train = []\n","for doc in tqdm(raw_docs_train):\n","    tokens = tokenizer.tokenize(doc)\n","    filtered = [word for word in tokens if word not in stop_words]\n","    processed_docs_train.append(\" \".join(filtered))\n","#end for\n","\n","processed_docs_test = []\n","for doc in tqdm(raw_docs_test):\n","    tokens = tokenizer.tokenize(doc)\n","    filtered = [word for word in tokens if word not in stop_words]\n","    processed_docs_test.append(\" \".join(filtered))\n","#end for\n","\n","print(\"tokenizing input data...\")\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n","tokenizer.fit_on_texts(processed_docs_train + processed_docs_test)  #leaky\n","word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\n","word_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\n","word_index = tokenizer.word_index\n","print(\"dictionary size: \", len(word_index))\n","\n","#pad sequences\n","word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n","word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EO_Khe4LDkl3"},"source":["#training params\n","batch_size =  8\n","num_epochs = 20\n","\n","#model parameters\n","num_filters = 8\n","embed_dim = 300 \n","weight_decay = 1e-4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ps-NZEo5D3tx"},"source":["#embedding matrix\n","print('preparing embedding matrix...')\n","words_not_found = []\n","nb_words = min(MAX_NB_WORDS, len(word_index))\n","embedding_matrix = np.zeros((nb_words, embed_dim))\n","for word, i in word_index.items():\n","    if i >= nb_words:\n","        continue\n","    embedding_vector = embeddings_index.get(word)\n","    if (embedding_vector is not None) and len(embedding_vector) > 0:\n","        # words not found in embedding index will be all-zeros.\n","        embedding_matrix[i] = embedding_vector\n","    else:\n","        words_not_found.append(word)\n","print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UlejikyCD6zU"},"source":["print(\"sample words not found: \", np.random.choice(words_not_found, 10))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d-__9pGuD_Vp"},"source":["#CNN architecture\n","print(\"training CNN ...\")\n","model = Sequential()\n","model.add(Embedding(nb_words, embed_dim,\n","          weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\n","model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n","model.add(MaxPooling1D(2))\n","model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n","model.add(GlobalMaxPooling1D())\n","model.add(Dropout(0.5))\n","model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n","model.add(Dense(2, activation='tanh'))  #multi-label (k-hot encoding)\n","\n","adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n","model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C3uyzNgbEDkG"},"source":["#define callbacks\n","early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n","callbacks_list = [early_stopping]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9DtRvYhlEFYK"},"source":["#model training\n","hist = model.fit(word_seq_train, y_train, batch_size=batch_size, epochs=num_epochs, callbacks=callbacks_list, validation_split=0.1, shuffle=True, verbose=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nNailDxPPhNU"},"source":["# ####################################\""]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AWCFrmC5Pfqx","executionInfo":{"status":"ok","timestamp":1624780925398,"user_tz":-60,"elapsed":4251,"user":{"displayName":"naim mhedhbi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFfR6yw4SzjRo72NjYLc0wwqC8NZ5-SEeMpcMU1A=s64","userId":"06089257256759509045"}},"outputId":"13f295eb-0a23-417a-da9a-abd5f06d2d91"},"source":["import warnings\n","warnings.filterwarnings('ignore')\n","import nltk\n","nltk.download('stopwords')\n","# Modules for data manipulation\n","import numpy as np\n","import pandas as pd\n","import re\n","\n","# Modules for visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sb\n","\n","# Tools for preprocessing input data\n","from bs4 import BeautifulSoup\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","# Tools for creating ngrams and vectorizing input data\n","from gensim.models import Word2Vec, Phrases\n","\n","# Tools for building a model\n","from sklearn.model_selection import train_test_split\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM, Dropout, Bidirectional\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing.sequence import pad_sequences\n","\n","# Tools for assessing the quality of model prediction\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","\n","import os\n","from sklearn.model_selection import KFold, cross_val_score, train_test_split\n","import pandas as pd\n","import os\n","import io\n","import numpy as np\n","import zipfile\n","import sys\n","import datetime\n","from sklearn.model_selection import train_test_split\n","import requests\n","import shutil\n","import keras\n","from keras import optimizers\n","from keras import backend as K\n","from keras import regularizers\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout, Flatten\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n","\n","from keras.preprocessing import sequence\n","from keras.preprocessing.text import Tokenizer\n","from keras.callbacks import EarlyStopping\n","\n","from tqdm import tqdm\n","from nltk.corpus import stopwords\n","from nltk.tokenize import RegexpTokenizer \n","import os, re, csv, math, codecs\n","\n","\n","\n","MAX_NB_WORDS = 100000\n","tokenizer = RegexpTokenizer(r'\\w+')\n","stop_words = set(stopwords.words('english'))\n","stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n","\n","from subprocess import check_output"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"miOmBv8HImH4","executionInfo":{"status":"ok","timestamp":1624780925399,"user_tz":-60,"elapsed":10,"user":{"displayName":"naim mhedhbi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFfR6yw4SzjRo72NjYLc0wwqC8NZ5-SEeMpcMU1A=s64","userId":"06089257256759509045"}}},"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load in \n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import io\n","from sklearn.feature_extraction.text import CountVectorizer\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n","from sklearn.model_selection import train_test_split\n","from keras.utils.np_utils import to_categorical\n","from sklearn.utils import resample\n","from sklearn.utils import shuffle\n","from sklearn.metrics import confusion_matrix,classification_report\n","import re\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"f0VqlBEcIw57","executionInfo":{"status":"ok","timestamp":1624780926309,"user_tz":-60,"elapsed":918,"user":{"displayName":"naim mhedhbi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFfR6yw4SzjRo72NjYLc0wwqC8NZ5-SEeMpcMU1A=s64","userId":"06089257256759509045"}}},"source":["def load_dataset(url: str, sep: str = ',', decode: str = 'utf-8', keep_default_na: bool = True, error_bad_lines: bool = True):\n","    def requests_csv(url: str, decode: str):\n","        return (requests.get(url).content).decode(decode)\n","    \n","    return pd.read_csv(io.StringIO(requests_csv(url, decode)),\n","                       sep=sep,\n","                       keep_default_na=keep_default_na,\n","                       error_bad_lines=error_bad_lines\n","                       )\n","url_dataset = \"https://media.githubusercontent.com/media/Y4rd13/datasets/main/upwork_dataset.csv\" \n","\n","from sklearn.model_selection import train_test_split\n","\n","data =  load_dataset(url_dataset, decode=\"cp1252\", keep_default_na=True, error_bad_lines=True)\n","#train_df = train_df.sample(2000)\n","data['jobText'] = data['jobText'].astype(str) \n","data['approved'] = data['approved'].astype(str)\n","#train_df, test_df = train_test_split(train_df, test_size = 0.2, random_state=42)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"fRJrtAEvPyig","executionInfo":{"status":"ok","timestamp":1624780926886,"user_tz":-60,"elapsed":582,"user":{"displayName":"naim mhedhbi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFfR6yw4SzjRo72NjYLc0wwqC8NZ5-SEeMpcMU1A=s64","userId":"06089257256759509045"}}},"source":["\n","def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r\"what's\", \"what is \", text)\n","    text = re.sub(r\"\\'s\", \" \", text)\n","    text = re.sub(r\"\\'ve\", \" have \", text)\n","    text = re.sub(r\"can't\", \"cannot \", text)\n","    text = re.sub(r\"n't\", \" not \", text)\n","    text = re.sub(r\"i'm\", \"i am \", text)\n","    text = re.sub(r\"\\'re\", \" are \", text)\n","    text = re.sub(r\"\\'d\", \" would \", text)\n","    text = re.sub(r\"\\'ll\", \" will \", text)\n","    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n","    text = re.sub('\\W', ' ', text)\n","    text = re.sub('\\s+', ' ', text)\n","    text = text.strip(' ')\n","    return text\n","\n","data['jobText'] = data['jobText'].map(lambda com : clean_text(com))"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"yz1D1i-fQP4J","executionInfo":{"status":"ok","timestamp":1624780928151,"user_tz":-60,"elapsed":1268,"user":{"displayName":"naim mhedhbi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFfR6yw4SzjRo72NjYLc0wwqC8NZ5-SEeMpcMU1A=s64","userId":"06089257256759509045"}}},"source":["\n","RE_PATTERNS = {\n","    ' fuck':\n","        [\n","            '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n","            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n","            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n","            '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n","            'feck ', ' fux ', 'f\\*\\*', \n","             'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck','fuk','fk'\n","        ],\n","    ' ass ':\n","        [\n","            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$','[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n","            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s'\n","        ],\n","    ' asshole ':\n","        [\n","            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole'\n","        ],\n","    ' bitch ':\n","        [\n","            'b[w]*i[t]*ch', 'b!tch',\n","            'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n","            'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h'\n","        ],\n","    ' bastard ':\n","        [\n","            'ba[s|z]+t[e|a]+rd'\n","        ],\n","    ' trans gender':\n","        [\n","            'transgender'\n","        ],\n","    ' gay ':\n","        [\n","            'gay' , 'g4y'\n","        ],\n","    ' cock ':\n","        [\n","            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n","            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n","        ],\n","    ' dick ':\n","        [\n","            ' dick[^aeiou]', 'deek', 'd i c k','dik'\n","        ],\n","    ' suck ':\n","        [\n","             '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'suck', '5uck', 's u c k'\n","        ],\n","    ' cunt ':\n","        [\n","            'cunt', 'c u n t'\n","        ],\n","    ' bullshit ':\n","        [\n","            'bullsh\\*t', 'bull\\$hit'\n","        ],\n","    ' idiot ':\n","        [\n","            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)',  'i d i o t'\n","                                                                                     \n","        ],\n","    ' dumb ':\n","        [\n","            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n","        ],\n","    ' shit ':\n","        [\n","            'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t'\n","        ],\n","    ' shit hole ':\n","        [\n","            'shythole'\n","        ],\n","    ' retard ':\n","        [\n","            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n","        ],\n","    ' rape ':\n","        [\n","            ' raped'\n","        ],\n","    ' dumb ass':\n","        [\n","            'dumbass', 'dubass'\n","        ],\n","    ' ass head':\n","        [\n","            'butthead'\n","        ],\n","    ' sex ':\n","        [\n","             's3x'\n","        ],\n","    ' nigger ':\n","        [\n","            'nigger', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n","        ],\n","    ' nigga ':\n","        [\n","            'niga', 'ni[g]+a', ' nigg[a]+'\n","        ],\n","    ' shut the fuck up':\n","        [\n","            'stfu'\n","        ],\n","    ' pussy ':\n","        [\n","            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses'\n","        ],\n","    ' faggot ':\n","        [\n","            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n","            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n","        ],\n","    ' motherfucker':\n","        [\n","            ' motha ', ' motha f', ' mother f', 'motherucker',\n","        ],\n","    ' whore ':\n","        [\n","             'w h o r e'\n","        ],\n","}\n","\n","\n","for target,patterns in RE_PATTERNS.items():\n","  for pat in patterns:\n","    data['jobText'] = data['jobText'].map(lambda x : re.sub(pat,target,x))"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":289},"id":"A694wmK0I7Ms","executionInfo":{"status":"ok","timestamp":1624780928153,"user_tz":-60,"elapsed":13,"user":{"displayName":"naim mhedhbi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFfR6yw4SzjRo72NjYLc0wwqC8NZ5-SEeMpcMU1A=s64","userId":"06089257256759509045"}},"outputId":"84334bb2-a2e8-4b8b-ba46-44db73b46c2e"},"source":["data['jobText'] = data['jobText'].apply(lambda x: x.lower())\n","# removing special chars\n","data['jobText'] = data['jobText'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n","data['jobText'] = data['jobText'].str.replace('rt','')\n","data.head()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>jobRegion</th>\n","      <th>jobText</th>\n","      <th>jobExperienceLevel</th>\n","      <th>jobType</th>\n","      <th>clientLocation</th>\n","      <th>clientRating</th>\n","      <th>approved</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Web Design &amp; Development Team (on-going work)</td>\n","      <td>Worldwide</td>\n","      <td>we need a team that is capable of working with...</td>\n","      <td>Expert</td>\n","      <td>Complex project</td>\n","      <td>United States</td>\n","      <td>5.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Website development</td>\n","      <td>Worldwide</td>\n","      <td>we are looking for someone to build a website ...</td>\n","      <td>Expert</td>\n","      <td>One-time project</td>\n","      <td>United States</td>\n","      <td>5.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Auction Website to sell Household items</td>\n","      <td>Worldwide</td>\n","      <td>i am looking to hire a agency or freelancers w...</td>\n","      <td>Expert</td>\n","      <td>Complex project</td>\n","      <td>United States</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>App development (Android &amp; iOS)</td>\n","      <td>Worldwide</td>\n","      <td>i am looking for someone that can make a compl...</td>\n","      <td>Intermediate</td>\n","      <td>One-time project</td>\n","      <td>Sweden</td>\n","      <td>4.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Mobile App Development (Media playback app)</td>\n","      <td>Worldwide</td>\n","      <td>what the deliverable is an audio player mobile...</td>\n","      <td>Intermediate</td>\n","      <td>One-time project</td>\n","      <td>United States</td>\n","      <td>5.0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                           title  ... approved\n","0  Web Design & Development Team (on-going work)  ...        1\n","1                            Website development  ...        1\n","2        Auction Website to sell Household items  ...        0\n","3                App development (Android & iOS)  ...        0\n","4    Mobile App Development (Media playback app)  ...        0\n","\n","[5 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TZrpUeZwJEnY","executionInfo":{"status":"ok","timestamp":1624780928732,"user_tz":-60,"elapsed":588,"user":{"displayName":"naim mhedhbi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFfR6yw4SzjRo72NjYLc0wwqC8NZ5-SEeMpcMU1A=s64","userId":"06089257256759509045"}},"outputId":"07c9b217-f0a9-4dae-88f5-5be200c55403"},"source":["max_fatures = 5000\n","tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n","tokenizer.fit_on_texts(data['jobText'].values)\n","X = tokenizer.texts_to_sequences(data['jobText'].values)\n","X = pad_sequences(X)\n","\n","Y = pd.get_dummies(data['approved']).values\n","X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\n","print(X_train.shape,Y_train.shape)\n","print(X_test.shape,Y_test.shape)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["(3507, 885) (3507, 2)\n","(877, 885) (877, 2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fFw81sgCV9Pu","executionInfo":{"status":"ok","timestamp":1624780928733,"user_tz":-60,"elapsed":7,"user":{"displayName":"naim mhedhbi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFfR6yw4SzjRo72NjYLc0wwqC8NZ5-SEeMpcMU1A=s64","userId":"06089257256759509045"}}},"source":["from keras.optimizers import Adam"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qy2uP6H_JR3H","colab":{"base_uri":"https://localhost:8080/","height":392},"executionInfo":{"status":"ok","timestamp":1624780930071,"user_tz":-60,"elapsed":1344,"user":{"displayName":"naim mhedhbi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFfR6yw4SzjRo72NjYLc0wwqC8NZ5-SEeMpcMU1A=s64","userId":"06089257256759509045"}},"outputId":"2d14de9e-9386-4926-abd5-ed58625c2207"},"source":["embed_dim = 128\n","lstm_out = 100\n","\n","model = Sequential()\n","model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n","model.add(SpatialDropout1D(0.4))\n","model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(2,activation='softmax'))\n","model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n","print(model.summary())\n","\n","\n","\n","\"\"\"model = Sequential()\n","model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n","model.add(Bidirectional(LSTM(256, return_sequences=True)))\n","model.add(Dropout(0.2))\n","model.add(Bidirectional(LSTM(256)))\n","model.add(Dropout(0.2))\n","model.add(Dense(128, activation=\"relu\"))\n","model.add(Dropout(0.2))\n","model.add(Dense(64, activation=\"relu\"))\n","model.add(Dropout(0.2))\n","model.add(Dense(2, activation='sigmoid'))\n","\n","\n","adam_opt = Adam(learning_rate=0.01)\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\"\"\""],"execution_count":9,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 885, 128)          640000    \n","_________________________________________________________________\n","spatial_dropout1d (SpatialDr (None, 885, 128)          0         \n","_________________________________________________________________\n","lstm (LSTM)                  (None, 100)               91600     \n","_________________________________________________________________\n","dense (Dense)                (None, 2)                 202       \n","=================================================================\n","Total params: 731,802\n","Trainable params: 731,802\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'model = Sequential()\\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\\nmodel.add(Bidirectional(LSTM(256, return_sequences=True)))\\nmodel.add(Dropout(0.2))\\nmodel.add(Bidirectional(LSTM(256)))\\nmodel.add(Dropout(0.2))\\nmodel.add(Dense(128, activation=\"relu\"))\\nmodel.add(Dropout(0.2))\\nmodel.add(Dense(64, activation=\"relu\"))\\nmodel.add(Dropout(0.2))\\nmodel.add(Dense(2, activation=\\'sigmoid\\'))\\n\\n\\nadam_opt = Adam(learning_rate=0.01)\\n\\nmodel.compile(loss=\\'binary_crossentropy\\', optimizer=\\'adam\\', metrics = [\\'accuracy\\'])'"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"hBrsLJzUieE0","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1624780930072,"user_tz":-60,"elapsed":27,"user":{"displayName":"naim mhedhbi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFfR6yw4SzjRo72NjYLc0wwqC8NZ5-SEeMpcMU1A=s64","userId":"06089257256759509045"}},"outputId":"19aee339-cc4f-4555-fbe1-60cd895f05f5"},"source":["\"\"\"early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n","callbacks_list = [early_stopping]\"\"\""],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\\ncallbacks_list = [early_stopping]\""]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"3DgeqJjvikNC","executionInfo":{"status":"ok","timestamp":1624780930073,"user_tz":-60,"elapsed":22,"user":{"displayName":"naim mhedhbi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFfR6yw4SzjRo72NjYLc0wwqC8NZ5-SEeMpcMU1A=s64","userId":"06089257256759509045"}},"outputId":"edf70507-6442-4edb-e724-dbf9e59fa7ed"},"source":["\"\"\"hist = model.fit( = 32\n","model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs, callbacks=callbacks_list, shuffle=True, verbose=1)\"\"\""],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'hist = model.fit( = 32\\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs, callbacks=callbacks_list, shuffle=True, verbose=1)'"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"NC5pYJEJJVes","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7d785fd0-5a43-42f3-b5c5-28852bcfee3e"},"source":["batch_size = 3\n","model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","1169/1169 [==============================] - 1397s 1s/step - loss: 0.3817 - accuracy: 0.8295\n","Epoch 2/5\n"," 581/1169 [=============>................] - ETA: 11:43 - loss: 0.2267 - accuracy: 0.9168"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rmM_VeDoNG3D"},"source":["Y_pred = model.predict_classes(X_test,batch_size = batch_size)\n","df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':Y_pred})\n","df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n","print(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\n","print(classification_report(df_test.true, df_test.pred))"],"execution_count":null,"outputs":[]}]}