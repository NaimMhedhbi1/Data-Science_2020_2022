{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchnlp.encoders.text import SpacyEncoder, pad_tensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from blitz.modules import BayesianEmbedding, BayesianLSTM, BayesianLinear\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "df = pd.read_csv('data/Reviews.csv')\n",
    "\n",
    "#drop useless data\n",
    "df = df.drop(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
    "       'HelpfulnessDenominator', 'Time', 'Summary',], axis=1)\n",
    "\n",
    "#remove ambiguous 3 and 4 stars for balancing\n",
    "#\n",
    "df = df[df['Score'] != 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create labels and preprocess\n",
    "df['Score'] = df['Score'].apply(lambda i: 'positive' if i > 4 else 'negative')\n",
    "df['Text'] = df['Text'].apply(lambda x:x.lower())\n",
    "\n",
    "#set names for beautiful df\n",
    "df.columns = ['labels', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_positive = df[df['labels']=='positive'].index\n",
    "nbr_to_drop = len(df) - len(idx_positive)\n",
    "\n",
    "drop_indices = np.random.choice(idx_positive, nbr_to_drop, replace=False)\n",
    "df = df.drop(drop_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['labels'] =='negative').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_list = df['text'].tolist()\n",
    "labels_as_list = df['labels'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "encoder = SpacyEncoder(text_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "encoded_texts = []\n",
    "for i in tqdm(range(len(text_as_list))):\n",
    "    encoded_texts.append(encoder.encode(text_as_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lengths = [len(i) for i in tqdm(encoded_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_as_series = pd.Series(lengths)\n",
    "plt.title(\"Probability Density Function for text lengths\")\n",
    "sns.distplot(length_as_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pad_length = length_as_series.quantile(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pad_length = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "reviews = []\n",
    "labels = []\n",
    "\n",
    "for i in tqdm(range(len(encoded_texts))):\n",
    "    if len(encoded_texts[i]) < max_pad_length:\n",
    "        reviews.append(encoded_texts[i])\n",
    "        labels.append(1 if labels_as_list[i] == \"positive\" else 0)\n",
    "        \n",
    "assert len(reviews) == len(labels), \"The labels and feature lists should have the same time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "padded_dataset = []\n",
    "for i in tqdm(range(len(reviews))):\n",
    "    padded_dataset.append(pad_tensor(reviews[i], int(max_pad_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the final dataset:\n",
    "X = torch.stack(padded_dataset)\n",
    "y = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y==1).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=.25,\n",
    "                                                    random_state=42)\n",
    "\n",
    "X_train, y_train = torch.tensor(X_train), torch.tensor(y_train)\n",
    "X_test, y_test = torch.tensor(X_test), torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating network and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class CustomLSTM(nn.Module):\n",
    "    def __init__(self, input_sz, hidden_sz):\n",
    "        super().__init__()\n",
    "        self.input_sz = input_sz\n",
    "        self.hidden_size = hidden_sz\n",
    "        self.W = nn.Parameter(torch.Tensor(input_sz, hidden_sz * 4))\n",
    "        self.U = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz * 4))\n",
    "        self.bias = nn.Parameter(torch.Tensor(hidden_sz * 4))\n",
    "        self.init_weights()\n",
    "                \n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "         \n",
    "    def forward(self, x, \n",
    "                init_states=None):\n",
    "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
    "        bs, seq_sz, _ = x.size()\n",
    "        hidden_seq = []\n",
    "        if init_states is None:\n",
    "            h_t, c_t = (torch.zeros(bs, self.hidden_size).to(x.device), \n",
    "                        torch.zeros(bs, self.hidden_size).to(x.device))\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "         \n",
    "        HS = self.hidden_size\n",
    "        for t in range(seq_sz):\n",
    "            x_t = x[:, t, :]\n",
    "            # batch the computations into a single matrix multiplication\n",
    "            gates = x_t @ self.W + h_t @ self.U + self.bias\n",
    "            i_t, f_t, g_t, o_t = (\n",
    "                torch.sigmoid(gates[:, :HS]), # input\n",
    "                torch.sigmoid(gates[:, HS:HS*2]), # forget\n",
    "                torch.tanh(gates[:, HS*2:HS*3]),\n",
    "                torch.sigmoid(gates[:, HS*3:]), # output\n",
    "            )\n",
    "            c_t = f_t * c_t + i_t * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "        return hidden_seq, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(encoder.vocab)+1, 32)\n",
    "        self.lstm = CustomLSTM(32,32)#nn.LSTM(32, 32, batch_first=True)\n",
    "        self.fc1 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_ = self.embedding(x)\n",
    "        x_, (h_n, c_n) = self.lstm(x_)\n",
    "        x_ = (x_[:, -1, :])\n",
    "        x_ = self.fc1(x_)\n",
    "        return x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(ds_train, batch_size=128, shuffle=True)\n",
    "\n",
    "ds_test = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "test_loader = torch.utils.data.DataLoader(ds_test, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "classifier = Net().to(device)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.005)#0.002 dives 85% acc\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_bar = tqdm(range(10),\n",
    "                 desc=\"Training\",\n",
    "                 position=0,\n",
    "                 total=2)\n",
    "\n",
    "acc = 0\n",
    "\n",
    "for epoch in epoch_bar:\n",
    "    batch_bar = tqdm(enumerate(train_loader),\n",
    "                     desc=\"Epoch: {}\".format(str(epoch)),\n",
    "                     position=1,\n",
    "                     total=len(train_loader))\n",
    "    \n",
    "    for i, (datapoints, labels) in batch_bar:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        preds = classifier(datapoints.long().to(device))\n",
    "        loss = criterion(preds, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #acc = (preds.argmax(dim=1) == labels).float().mean().cpu().item()\n",
    "        \n",
    "        if (i + 1) % 50 == 0:\n",
    "            acc = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for  i, (datapoints_, labels_) in enumerate(test_loader):\n",
    "                    preds = classifier(datapoints_.to(device))\n",
    "                    acc += (preds.argmax(dim=1) == labels_.to(device)).float().sum().cpu().item()\n",
    "            acc /= len(X_test)\n",
    "\n",
    "        batch_bar.set_postfix(loss=loss.cpu().item(),\n",
    "                              accuracy=\"{:.2f}\".format(acc),\n",
    "                              epoch=epoch)\n",
    "        batch_bar.update()\n",
    "\n",
    "        \n",
    "    epoch_bar.set_postfix(loss=loss.cpu().item(),\n",
    "                          accuracy=\"{:.2f}\".format(acc),\n",
    "                          epoch=epoch)\n",
    "    epoch_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
