{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.model_selection import cross_validate","metadata":{"execution":{"iopub.status.busy":"2021-06-26T22:34:08.261233Z","iopub.execute_input":"2021-06-26T22:34:08.261598Z","iopub.status.idle":"2021-06-26T22:34:08.265038Z","shell.execute_reply.started":"2021-06-26T22:34:08.261566Z","shell.execute_reply":"2021-06-26T22:34:08.264221Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# reading csv file  \n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport nltk\nnltk.download('stopwords')\n# Modules for data manipulation\nimport numpy as np\nimport pandas as pd\nimport re\n\n# Modules for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\n\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# Tools for creating ngrams and vectorizing input data\nfrom gensim.models import Word2Vec, Phrases\n\n# Tools for building a model\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout, Bidirectional\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Tools for assessing the quality of model prediction\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nimport os\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nimport pandas as pd\nimport os\nimport io\nimport numpy as np\nimport zipfile\nimport sys\nimport datetime\nfrom sklearn.model_selection import train_test_split\nimport requests\nimport shutil\nimport keras\nfrom keras import optimizers\nfrom keras import backend as K\nfrom keras import regularizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, Flatten\nfrom keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\n\nfrom tqdm import tqdm\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer \nimport os, re, csv, math, codecs\n\n\n\n\nfrom subprocess import check_output\n\n\n\ndef load_dataset(url: str, sep: str = ',', decode: str = 'utf-8', keep_default_na: bool = True, error_bad_lines: bool = True):\n    def requests_csv(url: str, decode: str):\n        return (requests.get(url).content).decode(decode)\n    \n    return pd.read_csv(io.StringIO(requests_csv(url, decode)),\n                       sep=sep,\n                       keep_default_na=keep_default_na,\n                       error_bad_lines=error_bad_lines\n                       )\nurl_dataset = \"https://media.githubusercontent.com/media/Y4rd13/datasets/main/upwork_dataset.csv\" \n\nfrom sklearn.model_selection import train_test_split\n\ndata =  load_dataset(url_dataset, decode=\"cp1252\", keep_default_na=True, error_bad_lines=True)\n#train_df = train_df.sample(2000)\ndata['jobText'] = data['jobText'].astype(str) \ndata['approved'] = data['approved'].astype(int)\ntrain, test = train_test_split(df, test_size = 0.2, random_state=42)\n#Classes: ['N': 0, 'S': 1, 'V': 2, 'F': 3, 'Q': 4]\n# N- normal, S-supraventricular, V-ventricular, F-fusion, Q- unknown","metadata":{"execution":{"iopub.status.busy":"2021-06-26T22:50:34.725603Z","iopub.execute_input":"2021-06-26T22:50:34.725928Z","iopub.status.idle":"2021-06-26T22:50:35.336240Z","shell.execute_reply.started":"2021-06-26T22:50:34.725897Z","shell.execute_reply":"2021-06-26T22:50:35.335402Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub('\\W', ' ', text)\n    text = re.sub('\\s+', ' ', text)\n    text = text.strip(' ')\n    return text\n\ndata['jobText'] = data['jobText'].map(lambda com : clean_text(com))","metadata":{"execution":{"iopub.status.busy":"2021-06-26T22:34:08.581675Z","iopub.execute_input":"2021-06-26T22:34:08.581940Z","iopub.status.idle":"2021-06-26T22:34:09.021759Z","shell.execute_reply.started":"2021-06-26T22:34:08.581914Z","shell.execute_reply":"2021-06-26T22:34:09.020939Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"RE_PATTERNS = {\n    ' fuck':\n        [\n            '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n            '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n            'feck ', ' fux ', 'f\\*\\*', \n             'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck','fuk','fk'\n        ],\n    ' ass ':\n        [\n            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$','[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s'\n        ],\n    ' asshole ':\n        [\n            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole'\n        ],\n    ' bitch ':\n        [\n            'b[w]*i[t]*ch', 'b!tch',\n            'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n            'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h'\n        ],\n    ' bastard ':\n        [\n            'ba[s|z]+t[e|a]+rd'\n        ],\n    ' trans gender':\n        [\n            'transgender'\n        ],\n    ' gay ':\n        [\n            'gay' , 'g4y'\n        ],\n    ' cock ':\n        [\n            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n        ],\n    ' dick ':\n        [\n            ' dick[^aeiou]', 'deek', 'd i c k','dik'\n        ],\n    ' suck ':\n        [\n             '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'suck', '5uck', 's u c k'\n        ],\n    ' cunt ':\n        [\n            'cunt', 'c u n t'\n        ],\n    ' bullshit ':\n        [\n            'bullsh\\*t', 'bull\\$hit'\n        ],\n    ' idiot ':\n        [\n            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)',  'i d i o t'\n                                                                                     \n        ],\n    ' dumb ':\n        [\n            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n        ],\n    ' shit ':\n        [\n            'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t'\n        ],\n    ' shit hole ':\n        [\n            'shythole'\n        ],\n    ' retard ':\n        [\n            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n        ],\n    ' rape ':\n        [\n            ' raped'\n        ],\n    ' dumb ass':\n        [\n            'dumbass', 'dubass'\n        ],\n    ' ass head':\n        [\n            'butthead'\n        ],\n    ' sex ':\n        [\n             's3x'\n        ],\n    ' nigger ':\n        [\n            'nigger', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n        ],\n    ' nigga ':\n        [\n            'niga', 'ni[g]+a', ' nigg[a]+'\n        ],\n    ' shut the fuck up':\n        [\n            'stfu'\n        ],\n    ' pussy ':\n        [\n            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses'\n        ],\n    ' faggot ':\n        [\n            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n        ],\n    ' motherfucker':\n        [\n            ' motha ', ' motha f', ' mother f', 'motherucker',\n        ],\n    ' whore ':\n        [\n             'w h o r e'\n        ],\n}\n\n\nfor target,patterns in RE_PATTERNS.items():\n  for pat in patterns:\n    data['jobText'] = data['jobText'].map(lambda x : re.sub(pat,target,x))","metadata":{"execution":{"iopub.status.busy":"2021-06-26T22:34:09.023671Z","iopub.execute_input":"2021-06-26T22:34:09.024032Z","iopub.status.idle":"2021-06-26T22:34:11.017561Z","shell.execute_reply.started":"2021-06-26T22:34:09.023979Z","shell.execute_reply":"2021-06-26T22:34:11.016699Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"data['jobText'] = data['jobText'].apply(lambda x: x.lower())\n# removing special chars\ndata['jobText'] = data['jobText'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\ndata['jobText'] = data['jobText'].str.replace('rt','')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-26T22:34:11.022032Z","iopub.execute_input":"2021-06-26T22:34:11.024056Z","iopub.status.idle":"2021-06-26T22:34:11.159348Z","shell.execute_reply.started":"2021-06-26T22:34:11.024014Z","shell.execute_reply":"2021-06-26T22:34:11.158503Z"},"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"                                           title  jobRegion  \\\n0  Web Design & Development Team (on-going work)  Worldwide   \n1                            Website development  Worldwide   \n2        Auction Website to sell Household items  Worldwide   \n3                App development (Android & iOS)  Worldwide   \n4    Mobile App Development (Media playback app)  Worldwide   \n\n                                             jobText jobExperienceLevel  \\\n0  we need a team that is capable of working with...             Expert   \n1  we are looking for someone to build a website ...             Expert   \n2  i am looking to hire a agency or freelancers w...             Expert   \n3  i am looking for someone that can make a compl...       Intermediate   \n4  what the deliverable is an audio player mobile...       Intermediate   \n\n            jobType clientLocation  clientRating  approved  \n0   Complex project  United States           5.0         1  \n1  One-time project  United States           5.0         1  \n2   Complex project  United States           0.0         0  \n3  One-time project         Sweden           4.0         0  \n4  One-time project  United States           5.0         0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>jobRegion</th>\n      <th>jobText</th>\n      <th>jobExperienceLevel</th>\n      <th>jobType</th>\n      <th>clientLocation</th>\n      <th>clientRating</th>\n      <th>approved</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Web Design &amp; Development Team (on-going work)</td>\n      <td>Worldwide</td>\n      <td>we need a team that is capable of working with...</td>\n      <td>Expert</td>\n      <td>Complex project</td>\n      <td>United States</td>\n      <td>5.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Website development</td>\n      <td>Worldwide</td>\n      <td>we are looking for someone to build a website ...</td>\n      <td>Expert</td>\n      <td>One-time project</td>\n      <td>United States</td>\n      <td>5.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Auction Website to sell Household items</td>\n      <td>Worldwide</td>\n      <td>i am looking to hire a agency or freelancers w...</td>\n      <td>Expert</td>\n      <td>Complex project</td>\n      <td>United States</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>App development (Android &amp; iOS)</td>\n      <td>Worldwide</td>\n      <td>i am looking for someone that can make a compl...</td>\n      <td>Intermediate</td>\n      <td>One-time project</td>\n      <td>Sweden</td>\n      <td>4.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Mobile App Development (Media playback app)</td>\n      <td>Worldwide</td>\n      <td>what the deliverable is an audio player mobile...</td>\n      <td>Intermediate</td>\n      <td>One-time project</td>\n      <td>United States</td>\n      <td>5.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom xgboost import XGBClassifier\nimport pandas as pd\nimport os\n\n\ncv = CountVectorizer(max_features=4500, encoding=\"utf-8\",  \n      ngram_range = (1,2),  \n      token_pattern = \"[A-Za-z_][A-Za-z\\d_]*\")\nX = cv.fit_transform(data.jobText).toarray()\ny = data['approved']\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n      test_size=0.15, \n      random_state=0)\ncount_df = pd.DataFrame(X_train, columns=cv.get_feature_names())\n\ncount_df['etiket'] = y_train","metadata":{"execution":{"iopub.status.busy":"2021-06-26T22:34:11.163045Z","iopub.execute_input":"2021-06-26T22:34:11.165002Z","iopub.status.idle":"2021-06-26T22:34:12.566820Z","shell.execute_reply.started":"2021-06-26T22:34:11.164950Z","shell.execute_reply":"2021-06-26T22:34:12.566022Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"data.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T22:34:12.567976Z","iopub.execute_input":"2021-06-26T22:34:12.568323Z","iopub.status.idle":"2021-06-26T22:34:12.584002Z","shell.execute_reply.started":"2021-06-26T22:34:12.568290Z","shell.execute_reply":"2021-06-26T22:34:12.583086Z"},"trusted":true},"execution_count":69,"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"                                                  title  jobRegion  \\\n1785                             Mobile App Development  Worldwide   \n2602                                      App Developer  Worldwide   \n2457                                 Networking Car App  Worldwide   \n3816  Looking to discuss making a tracking app for s...  Worldwide   \n3762                                      app developer  Worldwide   \n\n                                                jobText jobExperienceLevel  \\\n1785  the deliverable is a simple intuitive mobile a...       Intermediate   \n2602  looking for a relatively inexpensive freelance...       Intermediate   \n2457  want to develop a new social media and network...             Expert   \n3816  this is a whole new concept i am looking to ha...             Expert   \n3762  i have a full figma design set for an app wher...       Intermediate   \n\n               jobType  clientLocation  clientRating  approved  \n1785  One-time project   United States          5.00         0  \n2602  One-time project   United States          0.00         0  \n2457   Ongoing project   United States          0.00         1  \n3816   Ongoing project  United Kingdom          4.81         0  \n3762  One-time project   United States          4.96         0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>jobRegion</th>\n      <th>jobText</th>\n      <th>jobExperienceLevel</th>\n      <th>jobType</th>\n      <th>clientLocation</th>\n      <th>clientRating</th>\n      <th>approved</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1785</th>\n      <td>Mobile App Development</td>\n      <td>Worldwide</td>\n      <td>the deliverable is a simple intuitive mobile a...</td>\n      <td>Intermediate</td>\n      <td>One-time project</td>\n      <td>United States</td>\n      <td>5.00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2602</th>\n      <td>App Developer</td>\n      <td>Worldwide</td>\n      <td>looking for a relatively inexpensive freelance...</td>\n      <td>Intermediate</td>\n      <td>One-time project</td>\n      <td>United States</td>\n      <td>0.00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2457</th>\n      <td>Networking Car App</td>\n      <td>Worldwide</td>\n      <td>want to develop a new social media and network...</td>\n      <td>Expert</td>\n      <td>Ongoing project</td>\n      <td>United States</td>\n      <td>0.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3816</th>\n      <td>Looking to discuss making a tracking app for s...</td>\n      <td>Worldwide</td>\n      <td>this is a whole new concept i am looking to ha...</td>\n      <td>Expert</td>\n      <td>Ongoing project</td>\n      <td>United Kingdom</td>\n      <td>4.81</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3762</th>\n      <td>app developer</td>\n      <td>Worldwide</td>\n      <td>i have a full figma design set for an app wher...</td>\n      <td>Intermediate</td>\n      <td>One-time project</td>\n      <td>United States</td>\n      <td>4.96</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"count_df.iloc[1:10,1:10]","metadata":{"execution":{"iopub.status.busy":"2021-06-26T22:34:12.585324Z","iopub.execute_input":"2021-06-26T22:34:12.585653Z","iopub.status.idle":"2021-06-26T22:34:12.598649Z","shell.execute_reply.started":"2021-06-26T22:34:12.585620Z","shell.execute_reply":"2021-06-26T22:34:12.597671Z"},"trusted":true},"execution_count":70,"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"   a app  a back  a backend  a basic  a better  a big  a bit  a bonus  a brief\n1      0       0          0        0         0      0      0        0        0\n2      0       0          0        0         0      0      0        0        0\n3      0       0          0        0         0      0      0        0        0\n4      0       0          0        0         0      0      0        0        0\n5      0       0          0        0         0      0      0        0        0\n6      0       0          0        0         0      0      0        0        0\n7      0       0          0        0         0      0      0        0        0\n8      0       0          0        0         0      0      0        0        0\n9      0       0          0        0         0      0      0        0        0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a app</th>\n      <th>a back</th>\n      <th>a backend</th>\n      <th>a basic</th>\n      <th>a better</th>\n      <th>a big</th>\n      <th>a bit</th>\n      <th>a bonus</th>\n      <th>a brief</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# fit model no training data\n\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\n\n# make predictions for test data\n\ny_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\n\n# evaluate predictions\n\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","metadata":{"execution":{"iopub.status.busy":"2021-06-26T22:34:12.600685Z","iopub.execute_input":"2021-06-26T22:34:12.601129Z","iopub.status.idle":"2021-06-26T22:34:55.226070Z","shell.execute_reply.started":"2021-06-26T22:34:12.601092Z","shell.execute_reply":"2021-06-26T22:34:55.225202Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"[22:34:13] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nAccuracy: 58.05%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# https://suatatan.com/posts/sklearn_xgboost_tc/","metadata":{}},{"cell_type":"code","source":"from os import chdir\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\nimport xgboost as xgb\nimport numpy as np\nimport seaborn as sns\n\n# Extracting the Dataset from Local Storage\n## training set\n\n# Pre Processing\nstop_words = stopwords.words('english')\nwnl = WordNetLemmatizer()\n\ndef preprocess(text_column):\n    \n# Remove link,user and special characters\n# And Lemmatize the words\nnew_review = []\n    for review in text_column:\n        \n# this text is a list of tokens for the review\n        text = re.sub(\"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\", ' ', str(review).lower()).strip()\n        text = [wnl.lemmatize(i) for i in text.split(' ') if i not in stop_words]\n        new_review.append(' '.join(text))\n        return new_review\n\ntrain['jobText'] = preprocess(train['jobText'])\ntest['jobText'] = preprocess(test['jobText'])\n\n# Data Exploration of Training Set\n## proportion of positive and negative reviews\nn_positive = 0\nn_negative = 0\nn_ratings = Counter(train['approved'])\nfor i in n_ratings.keys():\n    \n    if i == 1:\n              n_positive += n_ratings[i]\n    elif i < 1:\n              n_negative += n_ratings[i]\nplt.bar(['Positive', 'Negative'], [n_positive, n_negative])\nplt.title('Proportion of Positive and Negative Reviews')\nplt.ylabel('Count')\nplt.show()\n\n\n## counting the frequencies of words\ntrain_pos = train.loc[train['rating'] == 1, 'jobText']\ntrain_neg = train.loc[train['rating'] < 1, 'jobText']\n\ntotal_freq = dict()\nunique_freq = dict()\nfor review in train_pos:\n    \n    \n# this text is a list of tokens for the review\n    text = review.split(' ')\n    freq = nltk.FreqDist(text)\n    for token, count in freq.most_common(20):\n        \n        \n        if token in total_freq.keys():\n            \n            total_freq[token] += count\n            unique_freq[token] += 1\n        else:\n            total_freq[token] = count\n            unique_freq[token] = 1\n## top 20 words for positive reviews\nsorted(total_freq.items(), key = lambda x: x[1], reverse = True)[:20]\nsorted(unique_freq.items(), key = lambda x: x[1], reverse = True)[:20]\n\ntotal_freq = dict()\nunique_freq = dict()\nfor review in train_neg:\n    \n# this text is a list of tokens for the review\n    text = review.split(' ')\n    freq = nltk.FreqDist(text)\n    for token, count in freq.most_common(20):\n        \n        if token in total_freq.keys():\n            \n            total_freq[token] += count\n            unique_freq[token] += 1\n        else:\n            \n            total_freq[token] = count\n            unique_freq[token] = 1\n## top 20 words for positive reviews\nsorted(total_freq.items(), key = lambda x: x[1], reverse = True)[:20]\nsorted(unique_freq.items(), key = lambda x: x[1], reverse = True)[:20]\n\n# Further Pre Processing\n## splitting into X and y\n## creating labels\ntrain_labels = []\nfor rating in train['approved']:\n    \n    if rating == 1 :\n        \n        train_labels.append(1)\n    elif rating < 1:\n        train_labels.append(0)\n    else:\n        train_labels.append(-1)\n\ntest_labels = []\nfor rating in test['approved']:\n    \n    if rating ==1 :\n        test_labels.append(1)\n    elif rating < 1:\n        test_labels.append(0)\n    else:\n        \n        test_labels.append(-1)\n              \n## creating train_x and test_x\ntrain_x = train['jobText']\ntest_x = test['jobText']\n\"\"\"\nWhat CountVectorizer does:\nIt creates one very large matrix with one column for every unique word in your corpus\n(where the corpus is all 50k reviews in our case). Then we transform each review into \none row containing 0s and 1s, where 1 means that the word in the corpus corresponding \nto that column appears in that review.\n\"\"\"\n## creates a matrix where the columns are the unique words and the each row is a review\n## where the corresponding element indicate the presence of that particular word\ncv = CountVectorizer(binary = True)\ncv.fit(train['review'])\ntrain_x = cv.transform(train_x)\ntest_x = cv.transform(test_x)\n\n# Looking at different ways to tokenize each review\n## This is for Word Count\ncv = CountVectorizer(binary = False)\ncv.fit(train_x)\nk_train_x = cv.transform(train_x)\nk_test_x = cv.transform(test_x)\n\n## Returns the TF-IDF for each Word\ntv = TfidfVectorizer(analyzer = 'word')\ntv.fit(train_x)\nk_train_x = tv.transform(train_x)\nk_test_x = tv.transform(test_x)\n\n## Returns the TF-IDF for each 2-gram Words\ntv = TfidfVectorizer(analyzer = 'word', ngram_range = (2, 2))\ntv.fit(train_x)\nk_train_x = tv.transform(train_x)\nk_test_x = tv.transform(test_x)\n\n## Returns the TF-IDF for each n-gram character within Words\ntv = TfidfVectorizer(analyzer = 'char_wb')\ntv.fit(train_x)\nk_train_x = tv.transform(train_x)\nk_test_x = tv.transform(test_x)\n\n# Logistic Regression Model Implementation\nmodel = LogisticRegression(max_iter = train_x.shape[1])\nmodel.fit(train_x, train_labels)\ny_pred = model.predict(test_x)\n\n## Log Reg Evaluation\naccuracy_score(test_labels, y_pred) # 0.8742464801683798\nf1_score(test_labels, y_pred) # 0.9150633505396528\n\n# Decision Tree Model Implementation\ntree_model = DecisionTreeClassifier() \ntree_model.fit(train_x, train_labels)\ny_pred_tree = tree_model.predict(test_x)\n\n## Decision Tree Evaluation\naccuracy_score(test_labels, y_pred_tree) # 0.8943335308662157\nf1_score(test_labels, y_pred_tree) # 0.927198749806417\n\n# XGBoost\nxgb_train_labels = []\nfor rating in train['rating']:\n       if rating >= 7:\n              xgb_train_labels.append(1)\n       elif rating <= 4:\n              xgb_train_labels.append(0)\n       else:\n              xgb_train_labels.append(None)\n\nxgb_test_labels = []\nfor rating in test['rating']:\n       if rating >= 7:\n              xgb_test_labels.append(1)\n       elif rating <= 4:\n              xgb_test_labels.append(0)\n       else:\n              xgb_test_labels.append(None)\nxgb_train = xgb.DMatrix(train_x, xgb_train_labels)\nxgb_test = xgb.DMatrix(test_x, xgb_test_labels)\nparam = {'eta': 0.75,\n         'max_depth': 50,\n         'objective': 'binary:logitraw'}\n## Training and Predicting\nxgb_model = xgb.train(param, xgb_train, num_boost_round = 30)\ny_pred_xgb = xgb_model.predict(xgb_test)\ny_pred_xgb = np.where(np.array(y_pred_xgb) > 0.5, 1, -1)\n\n# xgb Evaluation\naccuracy_score(test_labels, y_pred_xgb) # 0.9247195373643664\nf1_score(test_labels, y_pred_xgb) # 0.9483063452417704\nrecall_score(k_test_labels, y_pred_k_xgb)\nprecision_score(k_test_labels, y_pred_k_xgb)\nconfusion_matrix(k_test_labels, y_pred_k_xgb)\n\n# Exploratory Methods for XGBoost\n## looking at feature importance\nxgb_scores = xgb_model.get_score(importance_type = 'weight')\nxgb_scores = list(xgb_scores.items())\nxgb_scores.sort(key = lambda x: x[1], reverse = True)\nx = [i[0] for i in xgb_scores[:20]]\ny = [i[1] for i in xgb_scores[:20]]\n# plot\nplot = sns.barplot(y, x)\nplot.set_title('Top 20 Most Significant Variables', x = 0.66, weight = 'bold')\nplot.set_xlabel('No. of times feature is used to split the data across all trees')\n\nxgb_scores = xgb_model.get_score(importance_type = 'gain')\nxgb_scores = list(xgb_scores.items())\nxgb_scores.sort(key = lambda x: x[1], reverse = True)\nx = [i[0] for i in xgb_scores[:20]]\ny = [i[1] for i in xgb_scores[:20]]\n# plot\nplot = sns.barplot(y, x)\nplot.set_title('Top 20 Most Significant Variables', x = 0.66, weight = 'bold')\nplot.set_xlabel('How effective a feature is when used to split the data across all trees')\n\n## a more streamline way would be:\nxgb_model.feature_importances_","metadata":{},"execution_count":null,"outputs":[]}]}