{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://github.com/rsd16/SemEval2017-Task-4-Subtask-A-Classification-Sentiment-Analysis/blob/main/bert_model.py ","metadata":{"id":"o8vUcgth8IsK"}},{"cell_type":"code","source":"!pip install tensorflow==2.3 \n!pip install bert-for-tf2","metadata":{"id":"MMIDAzVJ8x0r","outputId":"670f36b5-d894-42eb-8c76-7b4b42bfaf44","execution":{"iopub.status.busy":"2021-09-25T22:07:06.980044Z","iopub.execute_input":"2021-09-25T22:07:06.980604Z","iopub.status.idle":"2021-09-25T22:07:21.213037Z","shell.execute_reply.started":"2021-09-25T22:07:06.980491Z","shell.execute_reply":"2021-09-25T22:07:21.212161Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport math\nimport datetime\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport bert\nfrom bert import BertModelLayer\nfrom bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\nfrom bert.tokenization.bert_tokenization import FullTokenizer\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator","metadata":{"id":"9rJRx7hm8Y8_","execution":{"iopub.status.busy":"2021-09-25T22:07:21.216696Z","iopub.execute_input":"2021-09-25T22:07:21.216927Z","iopub.status.idle":"2021-09-25T22:07:24.243020Z","shell.execute_reply.started":"2021-09-25T22:07:21.216899Z","shell.execute_reply":"2021-09-25T22:07:24.242290Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Label_Detection_Data:\n    data_column = 'pname'\n    label_column = 'category_id'\n\n    def __init__(self, train, test, tokenizer: FullTokenizer, classes, max_seq_len=64): # 192\n        self.tokenizer = tokenizer\n        self.max_seq_len = 0\n        self.classes = classes\n\n        train, test = map(lambda df: df.reindex(df[Label_Detection_Data.data_column].str.len().sort_values().index), [train, test])\n\n        ((self.x_train, self.y_train), (self.x_test, self.y_test)) = map(self._prepare, [train, test])\n\n        self.max_seq_len = min(self.max_seq_len, max_seq_len)\n        self.x_train, self.x_test = map(self._pad, [self.x_train, self.x_test])\n\n    def _prepare(self, df):\n        x, y = [], []\n        for _, row in tqdm(df.iterrows()):\n            text, label = row[Label_Detection_Data.data_column], row[Label_Detection_Data.label_column]\n            tokens = self.tokenizer.tokenize(text)\n            tokens = ['[CLS]'] + tokens + ['[SEP]']\n            token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n            self.max_seq_len = max(self.max_seq_len, len(token_ids))\n            x.append(token_ids)\n            y.append(self.classes.index(label))\n\n        return np.array(x), np.array(y)\n\n    def _pad(self, ids):\n        x = []\n        for input_ids in ids:\n            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n            x.append(np.array(input_ids))\n\n        return np.array(x)\n\n\ndef create_model(max_seq_len, bert_checkpoint_file):\n    with tf.io.gfile.GFile(bert_config_file, 'r') as reader:\n        bc = StockBertConfig.from_json_string(reader.read())\n        bert_params = map_stock_config_to_params(bc)\n        bert_params.adapter_size = None\n        bert = BertModelLayer.from_params(bert_params, name='bert')\n\n    input_ids = keras.layers.Input(shape=(max_seq_len, ), dtype='int32', name='input_ids')\n    bert_output = bert(input_ids)\n\n    X = tf.keras.layers.SpatialDropout1D(0.2)(bert_output)\n    X = tf.keras.layers.Conv1D(64, 5, activation='relu')(X)\n    X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2))(X)\n    X = tf.keras.layers.Dense(512, activation='relu')(X)\n    X = tf.keras.layers.Dropout(0.5)(X)\n    X = tf.keras.layers.Dense(512, activation='relu')(X)\n    logits = keras.layers.Dense(units=len(classes), activation='sigmoid')(X)\n    \n\n    model = keras.Model(inputs=input_ids, outputs=logits)\n    model.build(input_shape=(None, max_seq_len))\n\n    load_stock_weights(bert, bert_checkpoint_file)\n\n    return model\n\"\"\"\n    #cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)\n    #cls_out = keras.layers.Dropout(0.5)(cls_out)\n    #X = tf.keras.layers.LSTM(100, return_sequences=True)(bert_output)\n    #X = tf.keras.layers.GlobalMaxPool1D()(X)\n    X = tf.keras.layers.Dense(20, activation='relu')(bert_output)\n    #X = tf.keras.layers.Dropout(0.4)(X)\n    #X = tf.keras.layers.Dense(5, activation='softmax')(X)\n    #cls_out = keras.layers.LSTM(units=100, activation ='sigmoid')(bert_output)\n    #logits = keras.layers.Dense(units=768, activation='tanh')(cls_out)\n    #logits = keras.layers.Dropout(0.5)(cls_out)\n    logits = keras.layers.Dense(units=len(classes), activation='softmax')(X)\n\"\"\"\n\n\n\"\"\"X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(embedding_layer)\nX = tf.keras.layers.GlobalMaxPool1D()(X)\nX = tf.keras.layers.Dense(64, activation='relu')(X)\nX = tf.keras.layers.Dropout(0.2)(X)\nX = tf.keras.layers.Dense(5, activation='softmax')(X)\n\nmodel = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n\"\"\"\n\ndef remove_datapoints(x_train, y_train, x_test, y_test):\n    percentage = 0.75\n\n    def slash_datapoints(*args):\n        labels, counts = np.unique(args[1], return_counts=True)\n        bloodborne = []\n        for label in labels:\n            indices = np.array(np.where(args[1] == label)).tolist()[0]\n            num_indices = round(len(indices) * percentage)\n            dead_indices = np.random.choice(indices, num_indices, replace=False)\n            bloodborne.append(dead_indices)\n\n        bloodborne = np.concatenate(bloodborne).ravel()\n        temp0 = np.delete(args[0], bloodborne, axis=0)\n        temp1 = np.delete(args[1], bloodborne, axis=0)\n\n        return temp0, temp1\n\n    #print(x_train.shape)\n    #print(y_train.shape)\n    #print(x_test.shape)\n    #print(y_test.shape)\n\n    x_train, y_train = slash_datapoints(x_train, y_train)\n    x_test, y_test = slash_datapoints(x_test, y_test)\n    \"\"\" from imblearn.over_sampling import SMOTE\n    smote = SMOTE('minority')\n    x_train, y_train = smote.fit_sample(x_trai,y_trai)\"\"\"\n    \n    \n    #print(x_train.shape)\n    #print(y_train.shape)\n    #print(x_test.shape)\n    #print(y_test.shape)\n\n    return x_train, y_train, x_test, y_test\n","metadata":{"id":"lc1pPzTM8ggF","execution":{"iopub.status.busy":"2021-09-25T22:09:23.976024Z","iopub.execute_input":"2021-09-25T22:09:23.976345Z","iopub.status.idle":"2021-09-25T22:09:24.003313Z","shell.execute_reply.started":"2021-09-25T22:09:23.976311Z","shell.execute_reply":"2021-09-25T22:09:24.002489Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"    samples from the positive class.\n\n    def augment_text(df,samples=300,pr=0.2):\n        aug_w2v.aug_p=pr\n        new_text=[]\n     \n    ##selecting the minority class samples\n        df_n=df[df.target==1].reset_index(drop=True)\n\n    ## data augmentation loop\n        for i in tqdm(np.random.randint(0,len(df_n),samples)):\n        \n            text = df_n.iloc[i]['text']\n            augmented_text = aug_w2v.augment(text)\n            new_text.append(augmented_text)\n    \n    \n    ## dataframe\n        new=pd.DataFrame({'text':new_text,'target':1})\n        df=shuffle(df.append(new).reset_index(drop=True))\n        return df\n   \n    train = augment_text(train)","metadata":{}},{"cell_type":"markdown","source":"\"\"\"    from imblearn.over_sampling import SMOTE\n    smote = SMOTE('minority')\n    x_train, y_train = smote.fit_sample(x_trai,y_trai)\"\"\"","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/datam/train_datam.csv')\ntest = pd.read_csv('../input/datam/test_datam.csv')","metadata":{"id":"4Eq-0EX69Dl6","execution":{"iopub.status.busy":"2021-09-25T22:09:24.005466Z","iopub.execute_input":"2021-09-25T22:09:24.005835Z","iopub.status.idle":"2021-09-25T22:09:24.264347Z","shell.execute_reply.started":"2021-09-25T22:09:24.005798Z","shell.execute_reply":"2021-09-25T22:09:24.263527Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"bert_checkpoint_file = '../input/bertpretrained/multi_cased_L-12_H-768_A-12/multi_cased_L-12_H-768_A-12/bert_model.ckpt'\nbert_config_file = '../input/bertpretrained/multi_cased_L-12_H-768_A-12/multi_cased_L-12_H-768_A-12/bert_config.json'\n\ntokenizer = FullTokenizer(vocab_file='../input/bertpretrained/multi_cased_L-12_H-768_A-12/multi_cased_L-12_H-768_A-12/vocab.txt')\n\nclasses = train.category_id.unique().tolist()\n\ndata = Label_Detection_Data(train, test, tokenizer, classes, max_seq_len=30)\n\ndata.x_train, data.y_train, data.x_test, data.y_test = remove_datapoints(data.x_train, data.y_train, data.x_test, data.y_test)\n\n#print(data.max_seq_len)\n\nprint(data.x_train.shape)\nprint(data.x_test.shape)\n\nprint(data.y_train.shape)\nprint(data.y_test.shape)","metadata":{"id":"uzM2ji-U9GRB","outputId":"b2909be5-2389-41a8-9781-db8571ecb10e","execution":{"iopub.status.busy":"2021-09-25T22:09:24.265695Z","iopub.execute_input":"2021-09-25T22:09:24.265977Z","iopub.status.idle":"2021-09-25T22:10:34.480595Z","shell.execute_reply.started":"2021-09-25T22:09:24.265943Z","shell.execute_reply":"2021-09-25T22:10:34.478576Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model = create_model(data.max_seq_len, bert_checkpoint_file)\n\nmodel.compile(optimizer=keras.optimizers.Adam(1e-3), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n\nmodel.summary()\n\nhistory = model.fit(data.x_train, data.y_train,  validation_split=0.2, batch_size=256, epochs=10)\n\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='best')\n#plt.show()\nplt.savefig('bert_model-accuracy.png')\n\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(loc='best')\n#plt.show()\nplt.savefig('bert_model-loss.png')\n\n_, accuracy = model.evaluate(data.x_test, data.y_test)\nprint('Accuracy: ', accuracy)\n\nwith open('bert_model_results.txt', 'a') as file:\n    file.write('{}'.format(accuracy * 100))\n\n","metadata":{"id":"AaRnv2P879oM","outputId":"7943fefa-3809-46dc-ca8e-258bc3b2cc5e","execution":{"iopub.status.busy":"2021-09-25T22:10:34.482085Z","iopub.execute_input":"2021-09-25T22:10:34.482354Z"},"trusted":true},"execution_count":null,"outputs":[]}]}