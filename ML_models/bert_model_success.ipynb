{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_model_success.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8vUcgth8IsK"
      },
      "source": [
        "https://github.com/rsd16/SemEval2017-Task-4-Subtask-A-Classification-Sentiment-Analysis/blob/main/bert_model.py "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QnD6_I-8Gcr",
        "outputId": "2052749c-d02d-4bc3-a2de-a1e945f3c832"
      },
      "source": [
        "#access to the driver \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount = True) \n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Skd_WLz8ERt"
      },
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/datagram/data/train_df.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/datagram/data/test_df.csv')\n",
        "\n",
        "train_df['category_id'] = train_df['category_id'].astype(int) \n",
        "test_df['category_id'] = test_df['category_id'].astype(int)  \n",
        "\n",
        "train_df['pname'] = train_df['pname'].astype(str) \n",
        "test_df['pname'] = test_df['pname'].astype(str)  \n",
        "\n",
        "train_df = train_df.groupby('category_id').filter(lambda x : (x['category_id'].count()>=20).any())\n",
        "test_df = test_df.groupby('category_id').filter(lambda x : (x['category_id'].count()>=20).any())\n",
        "\n",
        "\n",
        "train_df.to_csv('train_datam.csv')\n",
        "!cp train_datam.csv \"/content/drive/MyDrive/datagram/data/\"\n",
        "\n",
        "test_df.to_csv('test_datam.csv')\n",
        "!cp test_datam.csv \"/content/drive/MyDrive/datagram/data/\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMIDAzVJ8x0r",
        "outputId": "670f36b5-d894-42eb-8c76-7b4b42bfaf44"
      },
      "source": [
        "!pip install tensorflow==2.3 \n",
        "!pip install bert-for-tf2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.3 in /usr/local/lib/python3.7/dist-packages (2.3.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (1.1.2)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (1.18.5)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (0.2.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (0.3.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (1.12.1)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (2.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (1.15.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (2.6.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (0.37.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (1.6.3)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (1.4.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (1.40.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (1.1.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (1.8.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (2.23.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3) (3.5.0)\n",
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.7/dist-packages (0.14.9)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from bert-for-tf2) (0.10.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.62.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rJRx7hm8Y8_"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import bert\n",
        "from bert import BertModelLayer\n",
        "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
        "from bert.tokenization.bert_tokenization import FullTokenizer\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc1pPzTM8ggF"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "class Label_Detection_Data:\n",
        "    data_column = 'pname'\n",
        "    label_column = 'category_id'\n",
        "\n",
        "    def __init__(self, train, test, tokenizer: FullTokenizer, classes, max_seq_len=64): # 192\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_len = 0\n",
        "        self.classes = classes\n",
        "\n",
        "        train, test = map(lambda df: df.reindex(df[Label_Detection_Data.data_column].str.len().sort_values().index), [train, test])\n",
        "\n",
        "        ((self.x_train, self.y_train), (self.x_test, self.y_test)) = map(self._prepare, [train, test])\n",
        "\n",
        "        self.max_seq_len = min(self.max_seq_len, max_seq_len)\n",
        "        self.x_train, self.x_test = map(self._pad, [self.x_train, self.x_test])\n",
        "\n",
        "    def _prepare(self, df):\n",
        "        x, y = [], []\n",
        "        for _, row in tqdm(df.iterrows()):\n",
        "            text, label = row[Label_Detection_Data.data_column], row[Label_Detection_Data.label_column]\n",
        "            tokens = self.tokenizer.tokenize(text)\n",
        "            tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "            token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "            self.max_seq_len = max(self.max_seq_len, len(token_ids))\n",
        "            x.append(token_ids)\n",
        "            y.append(self.classes.index(label))\n",
        "\n",
        "        return np.array(x), np.array(y)\n",
        "\n",
        "    def _pad(self, ids):\n",
        "        x = []\n",
        "        for input_ids in ids:\n",
        "            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n",
        "            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n",
        "            x.append(np.array(input_ids))\n",
        "\n",
        "        return np.array(x)\n",
        "\n",
        "\n",
        "def create_model(max_seq_len, bert_checkpoint_file):\n",
        "    with tf.io.gfile.GFile(bert_config_file, 'r') as reader:\n",
        "        bc = StockBertConfig.from_json_string(reader.read())\n",
        "        bert_params = map_stock_config_to_params(bc)\n",
        "        bert_params.adapter_size = None\n",
        "        bert = BertModelLayer.from_params(bert_params, name='bert')\n",
        "\n",
        "    input_ids = keras.layers.Input(shape=(max_seq_len, ), dtype='int32', name='input_ids')\n",
        "    bert_output = bert(input_ids)\n",
        "\n",
        "    cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)\n",
        "    cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
        "    logits = keras.layers.Dense(units=768, activation='tanh')(cls_out)\n",
        "    logits = keras.layers.Dropout(0.5)(logits)\n",
        "    logits = keras.layers.Dense(units=len(classes), activation='softmax')(logits)\n",
        "\n",
        "    model = keras.Model(inputs=input_ids, outputs=logits)\n",
        "    model.build(input_shape=(None, max_seq_len))\n",
        "\n",
        "    load_stock_weights(bert, bert_checkpoint_file)\n",
        "\n",
        "    return model\n",
        "\n",
        "def remove_datapoints(x_train, y_train, x_test, y_test):\n",
        "    percentage = 0.75\n",
        "\n",
        "    def slash_datapoints(*args):\n",
        "        labels, counts = np.unique(args[1], return_counts=True)\n",
        "        bloodborne = []\n",
        "        for label in labels:\n",
        "            indices = np.array(np.where(args[1] == label)).tolist()[0]\n",
        "            num_indices = round(len(indices) * percentage)\n",
        "            dead_indices = np.random.choice(indices, num_indices, replace=False)\n",
        "            bloodborne.append(dead_indices)\n",
        "\n",
        "        bloodborne = np.concatenate(bloodborne).ravel()\n",
        "        temp0 = np.delete(args[0], bloodborne, axis=0)\n",
        "        temp1 = np.delete(args[1], bloodborne, axis=0)\n",
        "\n",
        "        return temp0, temp1\n",
        "\n",
        "    #print(x_train.shape)\n",
        "    #print(y_train.shape)\n",
        "    #print(x_test.shape)\n",
        "    #print(y_test.shape)\n",
        "\n",
        "    x_train, y_train = slash_datapoints(x_train, y_train)\n",
        "    x_test, y_test = slash_datapoints(x_test, y_test)\n",
        "\n",
        "    #print(x_train.shape)\n",
        "    #print(y_train.shape)\n",
        "    #print(x_test.shape)\n",
        "    #print(y_test.shape)\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Eq-0EX69Dl6"
      },
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/datagram/data/train_datam.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/datagram/data/test_datam.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzM2ji-U9GRB",
        "outputId": "b2909be5-2389-41a8-9781-db8571ecb10e"
      },
      "source": [
        "bert_checkpoint_file = '/content/drive/MyDrive/bert_models/bert_model.ckpt'\n",
        "bert_config_file = '/content/drive/MyDrive/bert_models/bert_config.json'\n",
        "\n",
        "tokenizer = FullTokenizer(vocab_file='/content/drive/MyDrive/bert_models/vocab.txt')\n",
        "\n",
        "classes = train.category_id.unique().tolist()\n",
        "\n",
        "data = Label_Detection_Data(train, test, tokenizer, classes, max_seq_len=76)\n",
        "\n",
        "data.x_train, data.y_train, data.x_test, data.y_test = remove_datapoints(data.x_train, data.y_train, data.x_test, data.y_test)\n",
        "\n",
        "#print(data.max_seq_len)\n",
        "\n",
        "print(data.x_train.shape)\n",
        "print(data.x_test.shape)\n",
        "\n",
        "print(data.y_train.shape)\n",
        "print(data.y_test.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "161880it [01:03, 2539.94it/s]\n",
            "40002it [00:15, 2569.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(40474, 76)\n",
            "(10006, 76)\n",
            "(40474,)\n",
            "(10006,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaRnv2P879oM",
        "outputId": "7943fefa-3809-46dc-ca8e-258bc3b2cc5e"
      },
      "source": [
        "\n",
        "model = create_model(data.max_seq_len, bert_checkpoint_file)\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(1e-5), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(data.x_train, data.y_train, validation_split=0.2, batch_size=16, epochs=5)\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='best')\n",
        "#plt.show()\n",
        "plt.savefig('bert_model-accuracy.png')\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='best')\n",
        "#plt.show()\n",
        "plt.savefig('bert_model-loss.png')\n",
        "\n",
        "_, accuracy = model.evaluate(data.x_test, data.y_test)\n",
        "print('Accuracy: ', accuracy)\n",
        "\n",
        "with open('bert_model_results.txt', 'a') as file:\n",
        "    file.write('{}'.format(accuracy * 100))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done loading 196 BERT weights from: /content/drive/MyDrive/bert_models/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7fa68e8ec910> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
            "Unused weights from checkpoint: \n",
            "\tbert/embeddings/token_type_embeddings\n",
            "\tbert/pooler/dense/bias\n",
            "\tbert/pooler/dense/kernel\n",
            "\tcls/predictions/output_bias\n",
            "\tcls/predictions/transform/LayerNorm/beta\n",
            "\tcls/predictions/transform/LayerNorm/gamma\n",
            "\tcls/predictions/transform/dense/bias\n",
            "\tcls/predictions/transform/dense/kernel\n",
            "\tcls/seq_relationship/output_bias\n",
            "\tcls/seq_relationship/output_weights\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_ids (InputLayer)       [(None, 76)]              0         \n",
            "_________________________________________________________________\n",
            "bert (BertModelLayer)        (None, 76, 768)           108890112 \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 768)               590592    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 281)               216089    \n",
            "=================================================================\n",
            "Total params: 109,696,793\n",
            "Trainable params: 109,696,793\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "2024/2024 [==============================] - 1609s 795ms/step - loss: 5.6167 - accuracy: 0.0281 - val_loss: 5.6242 - val_accuracy: 0.0203\n",
            "Epoch 2/5\n",
            "1086/2024 [===============>..............] - ETA: 11:42 - loss: 5.6145 - accuracy: 0.0300"
          ]
        }
      ]
    }
  ]
}