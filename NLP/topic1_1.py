# -*- coding: utf-8 -*-
"""TOPIC1-1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15FHzvDqy5_u-xBOa6Z461Gur7bqDnBZ_
"""

!pip install eli5

!pip install tensorflow_addons

import numpy as np
import pandas as pd
import os
import sys
from google.colab import drive
import matplotlib.pyplot as plt
import math
import itertools

import random
import seaborn as sns
LOOK_AT = 10
SEED = 42
np.random.seed(SEED)
random.seed(SEED)

#mount google colab
drive.mount('/content/drive',force_remount = True)

full_music = pd.read_csv('/content/drive/MyDrive/Richard_FRL/Project Data/2021_ICM_Problem_D_Data/full_music_data.csv')

#full_music = full_music.groupby('artist_names').filter(lambda x : (x['artist_names'].count()>=100).any())

full_music['artist_names'].value_counts()

full_music.shape

full_music.columns

full_music.year.value_counts().sort_values().plot(kind = 'barh')

full_music.year.value_counts()
full_music['year'] = full_music['year'].astype(int)

del full_music['artist_names']  #useless 
del full_music['artists_id'] #useless 
del full_music['song_title (censored)']  #text column. 
del full_music['release_date'] #this column requires a lot of clening and it is redundant.

full_music['tempo']

# scale some parameter in the raw dataset to [0, 1]
import pandas as pd
from sklearn import preprocessing
parameter_columns = ['tempo','loudness', 'duration_ms','popularity']

min_max_scaler = preprocessing.MinMaxScaler()
scaled = min_max_scaler.fit_transform(full_music[parameter_columns])
full_music[[parameter for parameter in parameter_columns]] = scaled

full_music['tempo']

"""prepare the subdataframes on time intervals (decades)

# TOPIC1 : PART1

Divide the time (from 1950 to 2010) with decades interval, conduct a feature importance
analysis by XGboosting or other related machine learning algorithms for each period time and 
Visualize the results
"""

mask_1950 =(full_music['year'] >= 1950) & (full_music['year'] <= 1960)
df_1950 = full_music.loc[mask_1950]
del df_1950['year']

mask_1960 =(full_music['year'] > 1960) & (full_music['year'] <= 1970)
df_1960 = full_music.loc[mask_1960]
del df_1960['year']

mask_1970 =(full_music['year'] > 1970) & (full_music['year'] <= 1980)
df_1970 = full_music.loc[mask_1970]
del df_1970['year']

mask_1980 =(full_music['year'] > 1980) & (full_music['year'] <= 1990)
df_1980 = full_music.loc[mask_1980]
del df_1980['year']

mask_1990 =(full_music['year'] > 1990) & (full_music['year'] <= 2000)
df_1990 = full_music.loc[mask_1990]
del df_1990['year']

mask_2000 =(full_music['year'] > 2000) & (full_music['year'] <= 2010)
df_2000 = full_music.loc[mask_2000]
del df_2000['year']

#this is for part 2 of the topic 1  ; Will only focus on the timeframe from 1990 - 2020
mask_2020 =(full_music['year'] > 1990) & (full_music['year'] <= 2020)
df_2020 = full_music.loc[mask_2020]
del df_2020['year']

df_1950.shape

X_1950 =df_1950.iloc[:, : 13]
Y_1950 =df_1950['popularity']

X_1960 =df_1960.iloc[:, : 13]
Y_1960 =df_1960['popularity']

X_1970 =df_1970.iloc[:, : 13]
Y_1970 =df_1970['popularity']

X_1980 =df_1980.iloc[:, : 13]
Y_1980 =df_1980['popularity']

X_1990 =df_1990.iloc[:, : 13]
Y_1990 =df_1990['popularity']

X_2000 =df_2000.iloc[:, : 13]
Y_2000 =df_2000['popularity']

from xgboost import XGBClassifier 
from matplotlib import pyplot as plt 
from matplotlib.pyplot import figure

classifier = XGBClassifier()

"""# FOR 1950-1960"""

classifier.fit(X_1950, Y_1950) 
print(classifier.feature_importances_)
figure(figsize=(15, 4.5), dpi=80,)
plt.bar(X_1950.columns,classifier.feature_importances_)
plt.show()

"""# FOR 1960-1970"""

classifier.fit(X_1960, Y_1960) 
print(classifier.feature_importances_)
figure(figsize=(15, 4.5), dpi=80,)
plt.bar(X_1960.columns,classifier.feature_importances_)
plt.show()

"""# FOR 1970-1980"""

classifier.fit(X_1970, Y_1970) 
print(classifier.feature_importances_)
figure(figsize=(15, 4.5), dpi=80,)
plt.bar(X_1970.columns,classifier.feature_importances_)
plt.show()

"""# FOR 1980-1990"""

classifier.fit(X_1980, Y_1980) 
print(classifier.feature_importances_)
figure(figsize=(15, 4.5), dpi=80,)
plt.bar(X_1980.columns,classifier.feature_importances_)
plt.show()

"""# FOR 1990-2000"""

classifier.fit(X_1990, Y_1990) 
print(classifier.feature_importances_)
figure(figsize=(15, 4.5), dpi=80,)
plt.bar(X_1990.columns,classifier.feature_importances_)
plt.show()

"""# FOR 2000-2010"""

classifier.fit(X_2000, Y_2000) 
print(classifier.feature_importances_)
figure(figsize=(15, 4.5), dpi=80,)
plt.bar(X_2000.columns,classifier.feature_importances_)
plt.show()

"""we will download the df_2020 because of memeory issues (the sessions crashs) """

# e.g. save pandas output as csv
df_2020.to_csv('df_2020.csv')

!cp df_2020.csv "/content/drive/MyDrive/Richard_FRL/Project Data/2021_ICM_Problem_D_Data/"

"""# TOPIC1 : PART2"""

df_2020

df_2020['num_followers'] = np.random.uniform(0,1, df_2020.shape[0])

df_2020['num_followers'].isnull().values.any() #just to check if there is any nan value in the new column
                                              # I set the fake variables between 0 and 1 randomly to avoid the scaling part. but with real data we need few lines of code to scale the variable.

del df_2020['popularity'] 
#we removed this because it is not a feature of music. 
#Also , this time in the assignement , popularity in the second part is defined as the number of followers on the Social Media platforms. so our target variable is the new variable ((num_followers")) instead of the variable ((popularity)) that was used in the previous part.

df_2020.shape

X_2020 =df_2020.iloc[:, : 13]
Y_2020 =df_2020['num_followers']

classifier.fit(X_2020, Y_2020) 
print(classifier.feature_importances_)
figure(figsize=(15, 4.5), dpi=80,)
plt.bar(X_2020.columns,classifier.feature_importances_)
plt.show()